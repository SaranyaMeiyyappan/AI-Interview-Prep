## Prompt
Prompt is the structured input we send to an LLM. It can include instructions, examples, constraints, and contextual information that guide how the model should respond.

From a backend perspective, prompt design is similar to defining a method contract — clearer input leads to more predictable output.

## Response
Response is the output generated by the LLM based on the prompt. The response can be free-form text or structured data, depending on how the prompt is designed.

> [!Important]
> “The same prompt can produce different responses unless we control model parameters and structure the prompt carefully.”
>
> ***Example***
>
> **Prompt:**
> “Summarize the following error log in 3 bullet points and suggest possible root causes.”
>
> **Response:**
> A concise summary and suggested causes generated by the model.

> [!Note]
> “Can the response be trusted as-is?”
>
> “No. In enterprise applications, we treat LLM responses as suggestions and apply validation or human review depending on the use case.”
